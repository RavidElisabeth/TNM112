{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "14wALOy_u0WH"
   },
   "source": [
    "# TNM112 -- Teaching Session 2 \n",
    "\n",
    "## Introduction to Keras\n",
    "\n",
    "In this notebook, we will train some machine learning models using Keras framework.\n",
    "We will also use Scikit-learn for data preprocessing and evaluation. Kindly check the\n",
    "following links for more details:\n",
    "\n",
    "1. https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing\n",
    "2. https://keras.io/getting_started/\n",
    "3. https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "udAfMpcgu0WJ"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import tensorflow\n",
    "import sqlite3\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from pandas.plotting import parallel_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXUYDK1Ju0WM"
   },
   "source": [
    "# 1. IRIS Flowers Classifier\n",
    "\n",
    "In this section, we will train a neural network to classify IRIS flowers dataset. Here, weâ€™ll walk through the implementation steps so you can use this information to train another classifier in the next section. \n",
    "\n",
    "## 1.1. Load the dataset\n",
    "\n",
    "For the dataset, we use IRIS dataset. IRIS dataset consist of 150 datapoints with four input features [\"Sepal Length\", \"Sepal Width\", \"Petal Length\", \"Petal Width\"] and three output classes [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]. You can [download](http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data) the iris flowers dataset from the UCI Machine Learning repository.\n",
    "\n",
    "In the cell below, the IRIS dataset is loaded using [Pandas](https://pandas.pydata.org/) and visualized using parallel-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "Qb6LFTOGCe2O",
    "outputId": "7d7100b9-0374-4162-fdbc-950abc758b69"
   },
   "outputs": [],
   "source": [
    "# Load data from URL using Pandas\n",
    "csv_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data'\n",
    "col_names = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm','Species']\n",
    "data =  pd.read_csv(csv_url, names = col_names)\n",
    "\n",
    "#Do Parallel Plot\n",
    "parallel_coordinates(data, 'Species', color=('#FF0000', '#0000FF', '#FFFF00'))\n",
    "plt.figure()\n",
    "\n",
    "# Display information about the dataset\n",
    "print('=====================================================================================================')\n",
    "print('First five rows in the dataset:\\n', data.head(5))\n",
    "print('=====================================================================================================')\n",
    "print('Information about Data:')\n",
    "print(data.info())\n",
    "\n",
    "# Split dataset into input features and class labels\n",
    "Y = data['Species']\n",
    "X = data.drop(['Species'], axis=1)\n",
    "print('=====================================================================================================')\n",
    "print(\"Shape of Input  features: {}\".format(X.shape))\n",
    "print(\"Shape of Output features: {}\".format(Y.shape))\n",
    "print('=====================================================================================================')\n",
    "print(\"Check the number of datapoints for each class label:\")\n",
    "print(Y.value_counts())\n",
    "\n",
    "#One hot encode the class labels\n",
    "lbl_clf = LabelEncoder()\n",
    "Y_encoded = lbl_clf.fit_transform(Y)\n",
    "Y_final = tensorflow.keras.utils.to_categorical(Y_encoded)\n",
    "print('=====================================================================================================')\n",
    "print(\"Therefore, our final shape of output feature will be {}\".format(Y_final.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zG1MnN9lu0WY"
   },
   "source": [
    "## 1.2. Data Splitting and Normalization\n",
    "\n",
    "From the dataset details, you can see that the IRIS dataset contains 150 samples. In the cell below, the data is split into two sets, with 75% of the samples for the training set and the remaining 25% for the test set.\n",
    "\n",
    "Next, the input features are normalized using `StandardScaler` from scikit-learn. Normalization adjusts the feature values to have a mean of zero and a standard deviation of one, helping to improve the model's performance and training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uC2IMkwBu0WY",
    "outputId": "af301be9-e0b4-4d90-a8e3-5bb26f632979"
   },
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "#Split the dataset into train and test set using train_test_split() from sklearn\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y_final, test_size=0.25, random_state=seed, stratify=Y_encoded, shuffle=True)\n",
    "\n",
    "#Normalize the dataset using StandardScaler() from sklearn\n",
    "std_clf = StandardScaler()\n",
    "x_train_std = std_clf.fit_transform(x_train)\n",
    "x_test_std = std_clf.transform(x_test)\n",
    "\n",
    "print(\"Training Input shape\\t: {}\".format(x_train_std.shape))\n",
    "print(\"Testing Input shape\\t: {}\".format(x_test_std.shape))\n",
    "print(\"Training Output shape\\t: {}\".format(y_train.shape))\n",
    "print(\"Testing Output shape\\t: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Defining the Neural Network Architecture\n",
    "\n",
    "In the cell below, a neural network is defined using Keras. This architecture includes two hidden layers and is specifically designed to classify the IRIS dataset.\n",
    "\n",
    "**Network Architecture:**\n",
    "\n",
    "   1. **`First Hidden Layer:`** This layer has 10 neurons with ReLU activation, taking in the 4-dimensional input. Kernel is initialized with Normal distribution and uses L2 regularization (l2=0.01) to help reduce overfitting.\n",
    "\n",
    "   2. **`Batch Normalization:`** Applied after the first hidden layer to standardize its output.\n",
    "\n",
    "   3. **`Dropout Layer:`** 30% of the neurons are randomly dropped during training, which adds robustness by reducing reliance on specific neurons.\n",
    "\n",
    "   4. **`Second Hidden Layer:`** This layer has 5 neurons, also using ReLU activation.  Kernel is initialized with Normal distribution and uses L2 regularization (l2=0.01) to help reduce overfitting. \n",
    "\n",
    "   5. **`Batch Normalization:`** Applied after the second hidden layer to standardize its output.\n",
    "   \n",
    "   6. **`Dropout Layer:`** 30% of the neurons are randomly dropped during training, which adds robustness by reducing reliance on specific neurons.\n",
    "   \n",
    "   7. **`Output Layer:`** This layer has 3 neurons, one for each class in the IRIS dataset, and uses Softmax activation to produce a probability distribution across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "er6mJEBsu0WZ",
    "outputId": "25fe5689-7610-40ef-e296-e9f221d0a419"
   },
   "outputs": [],
   "source": [
    "#Define the neural network architecture. Check Keras documentation for more info\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "#Define a Sequential model\n",
    "model = keras.models.Sequential(name=\"MLP-1\")\n",
    "\n",
    "#First Hidden Layer with 10 neurons that takes 4 dimensional input value and relu activation\n",
    "model.add(keras.layers.Dense(10, input_dim=4, activation=tensorflow.nn.relu, kernel_initializer=\"normal\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                                name=\"hidden_layer_1\"))\n",
    "\n",
    "#Apply Batch Normalization to the output values of the first hidden layer\n",
    "model.add(keras.layers.BatchNormalization(name=\"batchnorm_1\"))\n",
    "\n",
    "#Adding Dropout to the first hidden layer with probability of 0.3\n",
    "model.add(keras.layers.Dropout(0.3,name=\"dropout_1\"))\n",
    "\n",
    "#Second Hidden Layer with 5 neurons that takes 10 dimensional input value from previous layer and relu activation\n",
    "model.add(keras.layers.Dense(5, activation = tensorflow.nn.relu, kernel_initializer=\"normal\",\n",
    "                                kernel_regularizer=keras.regularizers.l2(0.01),\n",
    "                                name=\"hidden_layer_2\"))\n",
    "\n",
    "#Apply Batch Normalization to the output values of the second hidden layer\n",
    "model.add(keras.layers.BatchNormalization(name=\"batchnorm_2\"))\n",
    "\n",
    "#Adding Dropout to the second hidden layer with probability of 0.3\n",
    "model.add(keras.layers.Dropout(0.3, name=\"dropout_2\"))\n",
    "\n",
    "#Output Layer with Softmax activation\n",
    "model.add(keras.layers.Dense(3, activation=tensorflow.nn.softmax,name=\"output_layer\"))\n",
    "\n",
    "#Once a model is \"built\", you can call its summary() method to display its contents\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f5XkZMPEf9k"
   },
   "source": [
    "## 1.4. Configuring and Training the Model\n",
    "\n",
    "In the cell below, the model will be trained based on the following hyperparameters.\n",
    "\n",
    "- **Optimizer:**  Adam optimizer\n",
    "\n",
    "- **Loss Function:** categorical_crossentropy\n",
    "\n",
    "- **Epochs:** 5\n",
    "\n",
    "- **Batch Size:** 7\n",
    "\n",
    "- **Metrics:** Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gM7gDUccEd7I",
    "outputId": "699b8981-5d18-42e7-ff4b-3cf44924b361"
   },
   "outputs": [],
   "source": [
    "#Set seed\n",
    "tensorflow.random.set_seed(42)\n",
    "\n",
    "#Configure the model for training. Define the hyperparatmeters: optimizer, loss and metrics\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "#Train the model\n",
    "iris_model = model.fit(x_train_std, y_train, epochs=5, batch_size=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hl2UKRpfEiiu"
   },
   "source": [
    "## 1.5. Evaluation on Test set\n",
    "In this cell, the trained model is evaluated on the test set and analyze its performance using key classification metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93a_Chs5u0Wa",
    "outputId": "b95d8cbb-3d02-4c61-cdd4-e3572464787b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#Evaluate the model on test set\n",
    "score = model.evaluate(x_test_std, y_test, verbose=0)\n",
    "\n",
    "#Score has two values. The first value represent loss and the second value represent the accuracy\n",
    "print(\"Test loss:      \", score[0])\n",
    "print(\"Test accuracy:  \", 100*score[1])\n",
    "\n",
    "#Get the model predictions on test set\n",
    "y_pred = model.predict(x_test_std)\n",
    "#Get the index of the highest value for each predictions (predicted class labels)\n",
    "y_pred = np.argmax(y_pred, axis = 1)\n",
    "#Convert the one hot vector to True class labels\n",
    "y_test_oh = np.argmax(y_test, axis =1)\n",
    "\n",
    "#Compute Precision, Recall, F1-Score and Accuracy of the model on the test set\n",
    "print('=====================================================================================================')\n",
    "print(classification_report(y_test_oh, y_pred, target_names=[\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]))\n",
    "print('=====================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6gz-tx1Ppbbf"
   },
   "source": [
    "## 2. Penguin Species Classifier\n",
    "\n",
    "In this section, we will train a neural network to classify three penguin species based on the features: `bill length`, `bill depth`, `flipper length` and `body mass`. Here, you will do the step-by-step implementation of the classifier.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1: Loading data\n",
    "\n",
    "For the dataset, we use Palmer Penguins dataset. The dataset consist of 344 datapoints with four input features : `bill length`, `bill depth`, `flipper length` and `body mass`. and three output classes `Adelie`, `Chinstrap` and `Gentoo`.\n",
    "\n",
    "In the cell below, the Palmer Penguins dataset is loaded as a DataFrame. You will visualize the dataset using parallel-plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = sns.load_dataset(\"penguins\")\n",
    "\n",
    "# Drop \"NaN\" values and column with text\n",
    "data = data.dropna()\n",
    "data = data.drop(columns=['island', 'sex'])\n",
    "\n",
    "# Split dataset into input features and class labels\n",
    "Y = data['species']\n",
    "X = data.drop(['species'], axis=1)\n",
    "\n",
    "#One hot encode the class labels\n",
    "lbl_clf = LabelEncoder()\n",
    "Y_encoded = lbl_clf.fit_transform(Y)\n",
    "Y_final = tensorflow.keras.utils.to_categorical(Y_encoded)\n",
    "\n",
    "print(f\"Length of the dataset:{len(data)}\\n\")\n",
    "print(data.head())\n",
    "\n",
    "# Task 1: Do a parallel-plot of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2: Data Splitting and Normalization\n",
    "\n",
    "From the dataset details, you can see that the Penguins dataset contains 333 samples (after removing \"NaNs\"). \n",
    "\n",
    "In the cell below, you will split the data, using 80% of the samples for the training set and the remaining 20% for the test set. Next, you will normalize the features using `StandardScaler` from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "\n",
    "# Task 1: Split the dataset into train and test set using train_test_split() from sklearn\n",
    "x_train, x_test, y_train, y_test = \n",
    "\n",
    "# Task 2: Normalize the dataset using StandardScaler() from sklearn\n",
    "x_train_std = \n",
    "x_test_std = \n",
    "\n",
    "print(\"Training Input shape\\t: {}\".format(x_train_std.shape))\n",
    "print(\"Testing Input shape\\t: {}\".format(x_test_std.shape))\n",
    "print(\"Training Output shape\\t: {}\".format(y_train.shape))\n",
    "print(\"Testing Output shape\\t: {}\".format(y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3: Defining the Neural Network Architecture\n",
    "\n",
    "In the cell below, you will define a neural network model using Keras. This architecture includes 3 hidden layers and is specifically designed to classify the IRIS dataset.\n",
    "\n",
    "Design the network based on the following information:\n",
    "\n",
    "   1. **`First Hidden Layer:`** This layer has 10 neurons with ReLU activation, taking in the 4-dimensional input. Kernel is initialized with He_Normal distribution and uses L2 regularization (l2=0.01) to help reduce overfitting.\n",
    "\n",
    "   2. **`Batch Normalization:`** Applied after the first hidden layer to standardize its output.\n",
    "\n",
    "   3. **`Dropout Layer:`** 25% of the neurons are randomly dropped during training, which adds robustness by reducing reliance on specific neurons.\n",
    "\n",
    "   4. **`Second Hidden Layer:`** This layer has 10 neurons, also using ReLU activation.  Kernel is initialized with He_Normal distribution and uses L1L2 regularization (experiment with L1 and L2 values) to help reduce overfitting. \n",
    "\n",
    "   5. **`Batch Normalization:`** Applied after the second hidden layer to standardize its output.\n",
    "   \n",
    "   6. **`Dropout Layer:`** 25% of the neurons are randomly dropped during training, which adds robustness by reducing reliance on specific neurons.\n",
    "   \n",
    "   7. **`Third Hidden Layer:`** This layer has 8 neurons, also using ReLU activation.  Kernel is initialized with He_Normal distribution and uses L1L2 regularization (experiment with L1 and L2 values) to help reduce overfitting. \n",
    "\n",
    "   8. **`Batch Normalization:`** Applied after the third hidden layer to standardize its output.\n",
    "   \n",
    "   9. **`Dropout Layer:`** 25% of the neurons are randomly dropped during training, which adds robustness by reducing reliance on specific neurons.\n",
    "   \n",
    "   10. **`Fourth Hidden Layer:`** This layer has 8 neurons with ReLU activation, taking in the 4-dimensional input. Kernel is initialized with He_Normal distribution and uses L2 regularization (l2=0.01) to help reduce overfitting.\n",
    "\n",
    "   11. **`Batch Normalization:`** Applied after the fourth hidden layer to standardize its output.\n",
    "\n",
    "   12. **`Dropout Layer:`** 25% of the neurons are randomly dropped during training, which adds robustness by reducing reliance on specific neurons.\n",
    "   \n",
    "   13. **`Output Layer:`** This layer has 3 neurons, one for each class in the Penguins dataset, and uses Softmax activation to produce a probability distribution across the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "\n",
    "# Define the Sequential model for the Penguin dataset\n",
    "model = keras.models.Sequential(name=\"Penguin_Classifier\")\n",
    "\n",
    "# Task 1: Create a neural network architecture mentioned in the cell above.\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4: Configuring and Training the Model\n",
    "\n",
    "In the cell below, we will configure the neural network model for training based on the following hyperparameters.\n",
    "\n",
    "- **Optimizer:**  AdamW optimizer\n",
    "\n",
    "- **Loss Function:** categorical_crossentropy\n",
    "\n",
    "- **Epochs:** 20\n",
    "\n",
    "- **Batch Size:** 16\n",
    "\n",
    "- **Metrics:** F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Set seed\n",
    "tensorflow.random.set_seed(42)\n",
    "\n",
    "# Task 1: Configure the model for training. Define the hyperparatmeters: optimizer, loss and metrics\n",
    "\n",
    "\n",
    "# Task 2: Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.5: Plot Loss Curve\n",
    "\n",
    "Visualize the training loss from the above training process using `matplotlib`. The training data can be accessed through `penguin_model.history`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Task 1: Plot the training loss curve\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.6: Evaluation on Test set\n",
    "\n",
    "In this cell, you will find the `Precision`, `Recall` and `F1-score` of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Task 1: Compute the precision, recall and F1-score of the trained model on test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aRX3mmChC2Jn"
   },
   "source": [
    "# 3. Live Plot and Predictions \n",
    "\n",
    "In this task, we will visualize the train and test accuracy along with the test set predictions after every batch in an epoch using live plot.\n",
    "\n",
    "## 3.1. Helper Function for Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmdeSBYEC5_i"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "\n",
    "def imgrid(x,y,yp,xx,yy):\n",
    "    ind = [i for i in range(x.shape[0])]\n",
    "    random.shuffle(ind)\n",
    "\n",
    "    plt.figure(figsize=(18,yy*2))\n",
    "    for i in range(xx*yy):\n",
    "        plt.subplot(yy,xx,i+1)\n",
    "        if x.shape[3]==1:\n",
    "            plt.imshow(x[ind[i],:,:,0],cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(x[ind[i],:,:,:])\n",
    "\n",
    "        if len(yp)>0:\n",
    "            plt.title('p=%d, gt=%d'%(yp[ind[i]],y[ind[i]]))\n",
    "        else:\n",
    "            plt.title('label=%d'%(y[ind[i]]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def live_plot(x, y, yp, acc, acc_test, batch, bs, N, xx, yy):\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    ind = [i for i in range(x.shape[0])]\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 10))\n",
    "    outer = gridspec.GridSpec(2, 1, hspace=0.2)\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(yy, xx,\n",
    "                    subplot_spec=outer[0], wspace=0.1, hspace=0.0)\n",
    "\n",
    "    for i in range(xx*yy):\n",
    "        ax = plt.Subplot(fig, inner[i])\n",
    "        if x.shape[3]==1:\n",
    "            ax.imshow(x[ind[i],:,:,0],cmap='gray')\n",
    "        else:\n",
    "            ax.imshow(x[ind[i],:,:,:])\n",
    "        if yp[ind[i]] == y[ind[i]]:\n",
    "            ax.set_title('Pred = %d'%(yp[ind[i]]), color='g')\n",
    "        else:\n",
    "            ax.set_title('Pred = %d'%(yp[ind[i]]), color='r')\n",
    "        ax.axis('off')\n",
    "        fig.add_subplot(ax)\n",
    "\n",
    "    inner = gridspec.GridSpecFromSubplotSpec(1, 1,\n",
    "                    subplot_spec=outer[1], wspace=0.0, hspace=0.1)\n",
    "    ax = plt.Subplot(fig, inner[0])\n",
    "    ax.plot(np.linspace(0,batch*bs/N,len(acc)),100.0*np.array(acc),label='Training')\n",
    "    ax.plot(np.linspace(0,batch*bs/N,len(acc_test)),100.0*np.array(acc_test),label='Test')\n",
    "    ax.plot(batch*bs/N,100*acc[-1],'o')\n",
    "    ax.plot(batch*bs/N,100*acc_test[-1],'o')\n",
    "    ax.legend()\n",
    "    ax.grid(1)\n",
    "    ax.set_xlim([0,np.maximum(1,batch*bs/N)])\n",
    "    ax.set_ylim([np.minimum(np.min(100.0*np.array(acc)),np.min(100.0*np.array(acc_test))),100])\n",
    "    ax.set_xlabel('Epochs')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    fig.add_subplot(ax)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.l_train = []\n",
    "        self.l_test = []\n",
    "        self.bs = 128\n",
    "        self.batch = 0\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        self.l_train.append(logs['accuracy'])\n",
    "        self.batch += 1\n",
    "\n",
    "        if np.mod(batch,10)==0:\n",
    "            score = model.evaluate(x_test, y_test, verbose=0)\n",
    "            self.l_test.append(score[1])\n",
    "\n",
    "            yp_test = np.argmax(model.predict(x_test[:24]),1)\n",
    "            live_plot(x_test,np.argmax(y_test[:24],1),yp_test,self.l_train,self.l_test,self.batch,self.bs,len(x_train)-self.bs,12,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g7RVKwFAC_u3"
   },
   "source": [
    "## 3.2. Load MNIST Data\n",
    "\n",
    "The MNIST dataset is a collection of handwritten digits, consisting of 60,000 training images and 10,000 test images, each 28x28 pixels in grayscale. It is commonly used for training and evaluating machine learning models, particularly for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yyhF3EjoDGda",
    "outputId": "c5112da6-c8db-40f7-8cdf-9dc6ff584d9e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# load MNIST dataset from keras\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize data\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "# One hot encoding of class labels\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Display images\n",
    "imgrid(x_train,np.argmax(y_train,1),[],12,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_eFCTbzD8vF"
   },
   "source": [
    "## 3.3. Model Definition and Training\n",
    "\n",
    "We will define Convolutional Neural Networks for this task, which will be covered in the lab 2 and the next teaching session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nBddXPVZD6dX",
    "outputId": "55e35b29-b46f-443f-e828-f1f09f9e9b37"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "#Define model architecture\n",
    "model = keras.Sequential(\n",
    "      [\n",
    "          layers.InputLayer(input_shape=(28,28,1)),\n",
    "          layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "          layers.Conv2D(8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "          layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "          layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "          layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "          layers.Flatten(),\n",
    "          layers.Dense(128, activation=\"relu\"),\n",
    "          layers.Dense(10, activation=\"softmax\"),\n",
    "      ])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 838
    },
    "id": "QuYkgzvWEHVj",
    "outputId": "35e1a466-e6c3-4863-c745-891f26668f72"
   },
   "outputs": [],
   "source": [
    "opt = keras.optimizers.Adam()\n",
    "\n",
    "#Configure the model for training\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "#Train the model for 1 Epoch\n",
    "model.fit(x_train, y_train, batch_size=128, epochs=1,\n",
    "          callbacks=[CustomCallback()],\n",
    "          validation_split=0.0, verbose=0);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0hxVvHzEqa9"
   },
   "source": [
    "## 3.4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fQQXEJDKEnJA",
    "outputId": "04d5bc30-87fa-4148-ef4f-bd4731bc666a"
   },
   "outputs": [],
   "source": [
    "#Displaying the training performance\n",
    "score = model.evaluate(x_train, y_train, verbose=0)\n",
    "print(\"Train loss:     \", score[0])\n",
    "print(\"Train accuracy: \", 100*score[1])\n",
    "\n",
    "#Displaying the test performance\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:      \", score[0])\n",
    "print(\"Test accuracy:  \", 100*score[1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl4mt",
   "language": "python",
   "name": "dl4mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
