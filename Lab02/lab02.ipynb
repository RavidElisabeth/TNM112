{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1da136-2f2b-44be-9113-dd9f3b379075",
   "metadata": {},
   "source": [
    "# TNM112 -- Lab 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7996b189-2fc8-4d10-9cbc-212dc44bdbb8",
   "metadata": {},
   "source": [
    "### Data generation\n",
    "Look through the \"data_generator.py\" code to understand how the dataset is generated and plotted.\n",
    "\n",
    "A dataset can be generated with a randomly selected subset of training images, and with a specified fraction used to formulate a validation set. Here, we use the MNIST dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c504f43a-e20d-4f78-affc-946ea332fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import data_generator\n",
    "\n",
    "data = data_generator.DataGenerator()\n",
    "data.generate(dataset='mnist', N_valid=0.1)\n",
    "data.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e916155-0299-4aee-9e67-b02346ba7b84",
   "metadata": {},
   "source": [
    "### Keras CNN\n",
    "A small CNN in Keras, trained with cross-entropy loss\n",
    "\n",
    "We use the functions in 'util.py' to evaluate the model and plot the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47c00a7-2bd6-498d-aad5-a720dab3bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import util\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=data.x_train.shape[1:]))\n",
    "model.add(layers.Conv2D(4, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Conv2D(8, kernel_size=(3, 3), activation='relu', padding='same'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(data.K, activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "log = model.fit(data.x_train, data.y_train_oh, batch_size=128, epochs=10, \n",
    "                validation_data=(data.x_valid, data.y_valid_oh), verbose=True)\n",
    "\n",
    "util.evaluate(model, data, final=True)\n",
    "util.plot_training(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4294a329-44df-41de-b00a-8c5a7042979f",
   "metadata": {},
   "source": [
    "### Our CNN\n",
    "\n",
    "#### Task 1\n",
    "Implement the following functions in cnn.py: 'activation', 'conv2d_layer', 'pool2d_layer', 'flatten_layer', 'dense_layer', and 'evaluate'. Additionally, compute the number of weights in the model, in the function 'setup_model'.\n",
    "\n",
    "For the 'activation' and 'dense_layer' functions you can use code that you wrote in Lab 1. For the 'evaluate' function you can use the code from Lab 1 for the accuracy. However, now we have trained with the cross-entropy loss, so you should evaluate this loss function through our CNN.\n",
    "\n",
    "When you have finished the implementation, you should be able to run this code and get the same result as with the Keras model above. However, our code is not optimized, nor parallelized, so it will be rather slow to process the full dataset. In order to facilitate comparisons when you implement the functions, please have a look in the next cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda8754b-5d52-4ff4-96cf-0a7ddfb561c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import cnn\n",
    "importlib.reload(cnn)\n",
    "\n",
    "# Get the weight matrices and biases of the trained Keras model\n",
    "W, b, lname = util.get_weights(model)\n",
    "\n",
    "# This is our implementation of a CNN, which we set to use the dataset we generated\n",
    "cnn = cnn.CNN(data, verbose=True)\n",
    "\n",
    "# Assign the weights and biases to the CNN and specify the activation function\n",
    "cnn.setup_model(W, b, lname, activation='relu')\n",
    "\n",
    "# Evaluate the model (accuracy on the training and test data)\n",
    "cnn.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96712fa5-5e17-4043-8112-7fb76ef3fc10",
   "metadata": {},
   "source": [
    "### Evaluation of our CNN layers\n",
    "\n",
    "#### Task 1\n",
    "As part of the implementation of the different layers in 'cnn.py', you can use the below code to compare the output of a randomly initialized Keras model and our CNN, for a single image. This is fast to compute, which will be convenient to check that your layers are correctly implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b91a9a4-5052-4237-9792-008f29ae25df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cnn\n",
    "importlib.reload(cnn)\n",
    "\n",
    "# Secify a test layer for comparison: 'conv', 'pool', 'flatten', or 'dense'\n",
    "test_layer = 'conv'\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Keras model and input layer\n",
    "model = keras.Sequential()\n",
    "model.add(layers.InputLayer(input_shape=data.x_train.shape[1:]))\n",
    "\n",
    "# Test conv layer\n",
    "#   - We apply normal initialization to the bias also,\n",
    "#     otherwise it would be difficult to see the impact\n",
    "#     of how you apply the bias (it is initialized to 0 by default)\n",
    "if test_layer == 'conv':\n",
    "    model.add(layers.Conv2D(4, kernel_size=(3, 3), \n",
    "                            activation='relu', padding='same',\n",
    "                            bias_initializer='normal'))\n",
    "    \n",
    "    # You could also test to add a second convolutional layer, to check\n",
    "    # that your network maps correctly from the output channels of the\n",
    "    # previous layer, to the channels of this layer\n",
    "    #model.add(layers.Conv2D(4, kernel_size=(3, 3), \n",
    "    #                        activation='relu', padding='same',\n",
    "    #                        bias_initializer='normal'))\n",
    "\n",
    "# Test pooling layer\n",
    "elif test_layer == 'pool':\n",
    "    model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Test flattening layer\n",
    "elif test_layer == 'flatten':\n",
    "    model.add(layers.Flatten())\n",
    "\n",
    "# Test dense layer\n",
    "#   - We need to first flatten the image input to apply a dense layer\n",
    "#   - Make sure that your flatten layer works before testing the dense layer\n",
    "elif test_layer == 'dense':\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation='softmax',\n",
    "                           bias_initializer='normal'))\n",
    "\n",
    "# Test data point (you could select any other image if you want)\n",
    "x = data.x_train[0]\n",
    "\n",
    "# Keras prediction. We need ta add an extra axis, \n",
    "# since the model can run over multiple datapoints. We \n",
    "# would need this in our model also if we use used the\n",
    "# 'feedforward' function instead of the 'feedforward_sample'\n",
    "y1 = model.predict(x[np.newaxis,:,:,:], verbose=False)\n",
    "\n",
    "# Prediction with our network\n",
    "W, b, lname = util.get_weights(model)  # Get the Keras model weights\n",
    "cnn = cnn.CNN(data)\n",
    "cnn.setup_model(W, b, lname, activation='relu') # Assign weights to our model\n",
    "y2 = cnn.feedforward_sample(x) # Feedforward of one single sample\n",
    "\n",
    "print('Evaluation of %s layer:'%test_layer)\n",
    "\n",
    "# Check that the dimensionality is correct. The Keras\n",
    "# model will have an extra first dimension due to the\n",
    "# batch processing\n",
    "print('\\tShape of Keras output:  ', y1.shape)\n",
    "print('\\tShape of our output:    ', y2.shape)\n",
    "\n",
    "# Print the absolute sum of output from the Keras\n",
    "# model and our model. These should be the same if the\n",
    "# implementation is correct\n",
    "print('\\tAbs sum of Keras model: ', np.sum(np.abs(y1)))\n",
    "print('\\tAbs sum of our model:   ', np.sum(np.abs(y2)))\n",
    "\n",
    "# Print the absolute difference between the Keras\n",
    "# model and our model. This should be very close\n",
    "# to 0 if your implementation is correct. It could be\n",
    "# a very minor difference due to numerical differences \n",
    "# (e.g., difference in precision used)\n",
    "print('\\tAbs difference:         ', np.sum(np.abs(y1-y2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7350b7-5f16-4bc0-b0e2-460f5fe10de5",
   "metadata": {},
   "source": [
    "### MNIST with limited training data\n",
    "#### Task 2\n",
    "In this experiment, we select a random subset from MNIST, with only 128 images. This is a very minimal dataset, but it is interesting to see how overfitting can be prevented with regularization strategies.\n",
    "\n",
    "Your task is to expand the given network with different regularization strategies. You are free to choose which combination of strategies you want to use, for example augmentation layers, dropout, weight decay, batch normalization, etc. You can also expand the network with more layers, strided convolutions, or add skip-connections to facilitate optimization. Moreover, you can experiment with the number of training epochs and the batch size.\n",
    "\n",
    "Also, note that now we have specified the Keras network in a sligthly different way. We explicitly formulate the input and output to the different layers. One motiviation for this is that it is easier if we want to, e.g., add skip-connections (checkout the layers.Add() layer in Keras).\n",
    "\n",
    "During your development, you test the performance on the validation set (util.evaluate with the flag final=False). When you have found a good setup, run evaluation on the test set (final=True). Do this for at least 5 runs and report the average and the variance across models. Since the subset of 128 images is randomly selected and the optimization is stochastic, you will get slightly different values each time. Thus, averaging is important to get a robust indication of your model's performance. You can do this manually, or you can setup a for-loop to run a sequence of trainings, where you log the evaluation results for each model.\n",
    "\n",
    "For the final results, you should aim at having an average accuracy of at least 90% (which is quite good considering that we are only training on 128 images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b6cdc-978c-4a6b-88af-1db77201c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A block of convolutional layers followed by max pooling\n",
    "def conv_block(x, N, channels, kernel_size, activation, padding='same'):\n",
    "    for i in range(N):\n",
    "        x = layers.Conv2D(channels, kernel_size=kernel_size, activation=activation, padding=padding)(x)\n",
    "    return layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    \n",
    "epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "# For setting up averaging over multiple training runs\n",
    "#K = 5\n",
    "#acc = np.zeros((K,2))\n",
    "#for k in range(K):\n",
    "\n",
    "# MNIST with 128 randomly selected training images\n",
    "data = data_generator.DataGenerator()\n",
    "data.generate(dataset='mnist', N_train=128)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "x      = layers.Input(shape=data.x_train.shape[1:])\n",
    "conv1  = conv_block(x, N=2, channels=8, kernel_size=(3,3), activation='relu', padding='same')\n",
    "conv2  = conv_block(conv1, N=2, channels=16, kernel_size=(3,3), activation='relu', padding='same')\n",
    "conv3  = conv_block(conv2, N=2, channels=32, kernel_size=(3,3), activation='relu', padding='same')\n",
    "flat1  = layers.Flatten()(conv3)\n",
    "\n",
    "dense1 = layers.Dense(512, activation='relu')(flat1)\n",
    "y = layers.Dense(data.K, activation='softmax')(dense1)\n",
    "\n",
    "model = keras.models.Model(inputs=x, outputs=y)\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "log = model.fit(data.x_train, data.y_train_oh, batch_size=batch_size, epochs=epochs, \n",
    "                validation_data=(data.x_valid, data.y_valid_oh), validation_freq=10,  # We don't need to evaluate at each epoch\n",
    "                verbose=True)\n",
    "\n",
    "util.evaluate(model, data, final=False)\n",
    "util.plot_training(log)\n",
    "\n",
    "# When you are finished with hyper-parameter tuning, you should evaluate\n",
    "# your model on the test set, and average over 5 trainings. If you want\n",
    "# to do this automatically, you can setup a foor loop and store each evaluation\n",
    "#acc[k,:] = util.evaluate(model, data, final=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ed3a6-4600-499e-bf50-6764a782ba92",
   "metadata": {},
   "source": [
    "### Tumor classification in digital pathology\n",
    "#### Task 3\n",
    "In this task, we will look at a more difficult task, using the PatchCamelyon dataset. This contains tissue samples from breast lymph nodes, which could either be healthy or contain tumor tissue, i.e. this is a binary classification problem. For this reason, it is valuable to also evaluate the AU-ROC, which can be passed to Keras as an evaluation metric, 'AUC'.\n",
    "\n",
    "You are provided with the training and validation set. These have been cropped to 32x32 pixels, to make it a bit less resource demanding to train. The test set is also provided, but without labels.\n",
    "\n",
    "Your task is to setup a Keras model to achieve the best possible performance on the validation set. You are free to choose how to do this, e.g., by means of regularization and network specifications. You can, for example, use your results from Task 2 as a starting point, but you should also think around how this problem differs from the MNIST classification. For example, in this case the orientation of images has no meaning, so you can randomly flip images both horizontally and vertically. You can also, e.g., explore augmentation by means of changing the image contrast and brightness.\n",
    "\n",
    "When you are finished with your development, you should run the 'util.pred_test' (at the bottom of the cell), to produce a CSV file with predictions of the test set. The exported CSV file should be uploaded to the Kaggle challenge, as described in the lab information. You need to train the model and export a CSV file 5 times, in order to have a good estimate for your model's performance. There are no requirements on how well your model should perform, as long as it is well above random guessing (50% accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39df1a6f-278b-4ad6-b8cc-498d6a2c6afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import util\n",
    "import data_generator\n",
    "    \n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "\n",
    "# Load the PatchCamyleon dataset\n",
    "# In this dataset, we don't have labels for the test set.\n",
    "# Do your development by monitoring the validation performance,\n",
    "# and when you are finished you will run predictions on the test\n",
    "# set and produce a CSV file that you can upload to Kaggle.\n",
    "data = data_generator.DataGenerator()\n",
    "data.generate(dataset='patchcam')\n",
    "data.plot()\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# TODO: Build your network here\n",
    "x = layers.Input(shape=data.x_train.shape[1:])\n",
    "flat1  = layers.Flatten()(x)\n",
    "y = layers.Dense(data.K, activation='softmax')(flat1)\n",
    "\n",
    "model = keras.models.Model(inputs=x, outputs=y)\n",
    "model.summary()\n",
    "\n",
    "opt = keras.optimizers.Adam()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy','AUC'])\n",
    "log = model.fit(data.x_train, data.y_train_oh, batch_size=batch_size, epochs=epochs, \n",
    "                validation_data=(data.x_valid, data.y_valid_oh), validation_freq=1,\n",
    "                verbose=True)\n",
    "\n",
    "util.evaluate(model, data)\n",
    "util.plot_training(log)\n",
    "\n",
    "# TODO: When you have finished your model development, you should \n",
    "# run inference on the test set and export a CSV file that can be \n",
    "# uploaded to Kaggle\n",
    "#util.pred_test(model, data, name='your_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
