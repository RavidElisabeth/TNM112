{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "600e124f",
   "metadata": {},
   "source": [
    "# TNM112 -- Teaching Session 1\n",
    "\n",
    "## Training Neural Network from Scratch\n",
    "\n",
    "In this teaching session, we will build on our learning from the `lab00`(`Introduction to Python`) and create a simple neural network from scratch and see how different components of neural network training works using [numpy](https://numpy.org/doc/stable/index.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c6bca9",
   "metadata": {},
   "source": [
    "### Task 1: Define a Single Neuron\n",
    "\n",
    "In this task,  we’ll create a single neuron that takes 4 features [x1, x2, x3, x4] as input and produces a single output.\n",
    "<img src=\"img/neuron.png\" alt=\"neuron\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Implement the above neuron in the next cell. Follow the equations given below to compute the value of Z, followed by the output of the neuron through ReLU activation function.\n",
    "\n",
    "$$\n",
    "                                    Z = X W^\\intercal + B =\\begin{bmatrix} x1 & x2 & x3 & x4 \\end{bmatrix} \\begin{bmatrix} w1 & w2 & w3 & w4 \\end{bmatrix}^\\intercal + [b1],\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "                                    output = relu(Z)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704d8a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(4) #Setting seed to reproduce the same answers\n",
    "\n",
    "# Step1: Define a function for neuron that accepts x and return z = Wxᵀ+b\n",
    "def neurons(x,W,B):\n",
    "    # Perform the operation xWᵀ + B and return Z\n",
    "    return np.matmul(x,np.transpose(W)) + B\n",
    "\n",
    "# Step2: Define a function that accepts an array and applies ReLU activation. (Hint check numpy documentation for maximum())\n",
    "def activation(z):\n",
    "\n",
    "    # Apply ReLU on the input variable and return the output\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "# Step3: Initialise Weights and biases from standard normal distribution (mean=0 & sd=1). Shape of W is (1x4) and B is (1)\n",
    "W = np.random.rand(1,4)\n",
    "B = np.random.rand(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9e9c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[-0.18040236  0.20218012  0.16951775 -0.71522547]]\n",
      "Weights: [[0.96702984 0.54723225 0.97268436 0.71481599]]\n",
      "Bias Term: [0.69772882]\n",
      "Z: [[0.2875465]]\n",
      "Output: [[0.2875465]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the neuron with sample input\n",
    "input_vector = np.random.normal(loc=0, scale=1, size=(1,4))  # Random input vector\n",
    "\n",
    "# Step4: Call the neurons function to get the value of Z\n",
    "Z = neurons(input_vector,W,B)\n",
    "\n",
    "# Step5: Call the activation function to get the final output\n",
    "output = activation(Z)\n",
    "\n",
    "# Print the input and output\n",
    "print(\"Input:\", input_vector)\n",
    "print(\"Weights:\", W)\n",
    "print(\"Bias Term:\", B)\n",
    "print(\"Z:\", Z)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b14a70b",
   "metadata": {},
   "source": [
    "### Task 2: Add another Neuron\n",
    "Now, let's add one more neuron to this layer. \n",
    "<img src=\"img/neuron2.png\" alt=\"neuron2\" width=\"400\"/>\n",
    "\n",
    "\n",
    "Implement the above layer of neuron in the next cell by following the equations given below. Rather than performing the calculations separately, we’ll concatenate the weights of the second neuron into the same weight matrix W as shown below:\n",
    "\n",
    "$$\n",
    "                                    Z = X W^\\intercal + B = \\begin{bmatrix} x1 & x2 & x3 & x4 \\end{bmatrix} \\begin{bmatrix} w11 & w21 & w31 & w41 \\\\ w12 & w22 & w32 & w42\\end{bmatrix}^\\intercal + \\begin{bmatrix}b1 & b2\\end{bmatrix},\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "                                    output = relu(Z)\n",
    "$$\n",
    "\n",
    "\n",
    "**Note:**\n",
    "In my setup, W  has dimensions [N x M] and X  has dimensions [L x M], where:\n",
    "\n",
    "- N is the number of neurons,\n",
    "- M is the number of input features for the layer,\n",
    "- L is the dataset length or number of inputs.\n",
    "\n",
    "Given these shapes, the calculation of z will be $ X W^\\intercal + B $. Some textbooks, however, may represent this calculation as $W X + B$. Ultimately, it’s still a matrix multiplication, so just ensure the matrix dimensions are aligned accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0697d322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Initialise Weights W = [[w11, w21, w31, w41],[w12, w22, w32, w42]] and bias B = [b1, b2] from standard normal distribution\n",
    "np.random.seed(4) #Setting seed to reproduce the same answers\n",
    "W = np.random.rand(2,4)\n",
    "B = np.random.rand(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "410f6c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [[ 0.61866969 -0.08798693  0.4250724   0.33225315]]\n",
      "Weights: [[0.96702984 0.54723225 0.97268436 0.71481599]\n",
      " [0.69772882 0.2160895  0.97627445 0.00623026]]\n",
      "Bias Term: [[0.25298236 0.43479153]]\n",
      "Z: [[1.45406626 1.2644995 ]]\n",
      "Output: [[1.45406626 1.2644995 ]]\n"
     ]
    }
   ],
   "source": [
    "# Testing the neurons with sample input\n",
    "input_vector = np.random.normal(loc=0, scale=1, size=(1,4))\n",
    "\n",
    "# Step2: Call the neurons function to get the value of Z\n",
    "Z = neurons(input_vector,W,B)\n",
    "\n",
    "# Step3: Call the activation function to get the final output\n",
    "output = activation(Z)\n",
    "\n",
    "# Print the input and output\n",
    "print(\"Input:\", input_vector)\n",
    "print(\"Weights:\", W)\n",
    "print(\"Bias Term:\", B)\n",
    "print(\"Z:\", Z)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294823d",
   "metadata": {},
   "source": [
    "### Task 3: Build a Neural Network\n",
    "\n",
    "Previously, we explored how to create a layer with multiple neurons. Now, let’s build a simple neural network as illustrated in the figure below:\n",
    "\n",
    "<img src=\"img/nn.png\" alt=\"nn\" width=\"700\"/>\n",
    "\n",
    "Write your implementation on the computations performed at each layer, as defined by the equations below. Step-by-step instructions are included in the comments to give you a clearer understanding.\n",
    "\n",
    "#### Layer 1:\n",
    "$$\n",
    "                                    Z₁ = X W₁^\\intercal + B₁ = \\begin{bmatrix} x1 & x2 & x3 & x4 \\end{bmatrix} \\begin{bmatrix} w11 & w21 & w31 & w41 \\\\ w12 & w22 & w32 & w42\\end{bmatrix}^\\intercal + \\begin{bmatrix}b1 & b2\\end{bmatrix},\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "                                    A₁ = \\begin{bmatrix}a1 & a2\\end{bmatrix} = relu(Z₁)\n",
    "$$\n",
    "\n",
    "#### Layer 2:\n",
    "$$\n",
    "                                    Z₂ = A₁ W₂^\\intercal + B₂ = \\begin{bmatrix} a1 & a2 \\end{bmatrix} \\begin{bmatrix} w13 & w23 \\\\ w14 & w24\\end{bmatrix}^\\intercal + \\begin{bmatrix}b3 & b4\\end{bmatrix},\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "                                    A₂ = \\begin{bmatrix}a3 & a4\\end{bmatrix} = relu(Z₂)\n",
    "$$\n",
    "\n",
    "#### Output Layer:\n",
    "$$\n",
    "                                    Z₃ = A₂ W₃^\\intercal + B₃ = \\begin{bmatrix} a3 & a4 \\end{bmatrix} \\begin{bmatrix} w3o & w4o \\end{bmatrix}^\\intercal + \\begin{bmatrix}bo\\end{bmatrix},\n",
    "$$ \n",
    "\n",
    "$$ \n",
    "                                    Output = \\sigma(Z₃) = \\frac{1}{1 + e^{-Z₃}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2ee4e242",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "#Let's define a class to construct the above neural network\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self,):\n",
    "        #Step1: Initialize the weights and biases\n",
    "        # Initialize weights and biases for Layer 1 from standard normal distribution (input to first hidden layer)\n",
    "        self.W1 = np.random.rand(2,4)\n",
    "        self.B1 = np.random.rand(1,2)\n",
    "        \n",
    "        # Initialize weights and biases for Layer 2 from standard normal distribution (first hidden to second hidden layer)\n",
    "        self.W2 = np.random.rand(2,2)\n",
    "        self.B2 = np.random.rand(1,2)\n",
    "        \n",
    "        # Initialize weights and biases for Output Layer from standard normal distribution (second hidden to output layer)\n",
    "        self.W3 = np.random.rand(1,2)\n",
    "        self.B3 = np.random.rand(1)\n",
    "    \n",
    "    #Step2: Define a function that accepts an array and applies ReLU activation. (Hint check numpy documentation for maximum())\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    #Step3: Define a function that accepts an array and applies Sigmoid activation. (Hint check numpy documentation for exponential())\n",
    "    def sigmoid(self, z):\n",
    "        return (1 / (1 + np.exp(-z)))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        #Step4: Implement the forward propagation\n",
    "        \n",
    "        # Perform the operation to get Z for layer 1\n",
    "        Z1 = np.matmul(X,np.transpose(self.W1)) + self.B1\n",
    "        # Pass Z1 to ReLU function\n",
    "        A1 = self.relu(Z1)\n",
    "\n",
    "        # Perform the operation to get Z for layer 2\n",
    "        Z2 = np.matmul(A1,np.transpose(self.W2)) + self.B2\n",
    "        # Pass Z2 to ReLU function\n",
    "        A2 = self.relu( Z2)\n",
    "        \n",
    "        # Perform the operation to get Z for the Output Layer\n",
    "        Z3 = np.matmul(A2,np.transpose(self.W3)) + self.B3\n",
    "        Z3 = Z3.flatten()\n",
    "        # Pass Z3 to Sigmoid function\n",
    "        output = self.sigmoid(Z3)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66627972",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: [0.87894972]\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the neural network\n",
    "network = MyNeuralNetwork()\n",
    "\n",
    "# Testing the neural network with sample input\n",
    "np.random.seed(4)\n",
    "X = np.random.normal(loc=0, scale=1, size=(1,4))\n",
    "\n",
    "# Step5: Perform the forward pass\n",
    "output = network.forward(X)\n",
    "print(\"Output:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1c9855",
   "metadata": {},
   "source": [
    "### Task 4 Dataset Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a88af5",
   "metadata": {},
   "source": [
    "Let us create a dataset suitable for training the neural network described above. We will generate the dataset similarly to what was done in `lab00`. Follow the steps outlined below:\n",
    "\n",
    "   1. `x1 = [128 x 4]` input arrays sampled from normal distribution with mean=1 and std=0.1. These input values belong to `Class 0` i.e. y1 is a 2D zero array of length 128.\n",
    "\n",
    "   2. `x2 = [128 x 4]` input arrays sampled from normal distribution with mean=0.5 and std=0.1. These input values belong to `Class 1` i.e. y2 is a 2D array of 1s with length 128.\n",
    "\n",
    "   3. Concatenate x1 and x2 into X (which is the input data) and y1 and y2 into Y (which is the output data). Apply a data shuffle through indices\n",
    "   \n",
    "Follow the instructions in the comments to create the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d018cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset:\n",
    "    def __init__(self):\n",
    "        # Generate data samples\n",
    "        self.data, self.labels = self._generate_data()\n",
    "\n",
    "    def _generate_data(self):\n",
    "        # Step1: Generate samples for Class 0 from normal distribution with mean=1 and std=0.1. (Check the instructions) \n",
    "        x1 = np.random.normal(loc=1, scale=0.1, size=(128, 4))  # shape (128, 4)\n",
    "        y1 = np.zeros((128, 1), dtype=int)  # labels for Class 0, shape (128, 1)\n",
    "\n",
    "        # Step2: Generate samples for Class 1 from normal distribution with mean=0.5 and std=0.1. (Check the instructions) \n",
    "        x2 = np.random.normal(loc=0.5, scale=0.1, size=(128, 4))  # shape (128, 4)\n",
    "        y2 = np.ones((128, 1), dtype=int)  # labels for Class 1, shape (128, 1)\n",
    "\n",
    "        # Step3: Concatenate data and labels. (Hint: Apply vertical stack of arrays)\n",
    "        data = np.vstack((x1, x2))\n",
    "        labels = np.vstack((y1, y2)).flatten()  \n",
    "\n",
    "        return data, labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "class DataLoader:\n",
    "    def __init__(self, dataset, batch_size=32, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(len(self.dataset))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for start_idx in range(0, len(self.dataset), self.batch_size):\n",
    "            batch_indices = self.indices[start_idx:start_idx + self.batch_size]\n",
    "            batch_data = [self.dataset[i] for i in batch_indices]\n",
    "            data, labels = zip(*batch_data)\n",
    "            yield np.array(data), np.array(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.dataset) / self.batch_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d64e2f24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Input Data: 256\n",
      "Number of Batches: 8\n"
     ]
    }
   ],
   "source": [
    "dataset = MyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "print(f'Number of Input Data: {len(dataset)}\\nNumber of Batches: {len(dataloader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381ede8e",
   "metadata": {},
   "source": [
    "### Task 5: Batch Computation\n",
    "\n",
    "From `task 1` to `task 3`, we have worked with the neural network using a single input `[1 x 4]` vector. Now, let's see if it can handle a batch of input vectors `[b.s x 4]`, where b.s is batch size (32 in this instance). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1ef73996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of first batch data:\n",
      " (16, 4)\n",
      "Shape of first batch labels:\n",
      " (16,)\n",
      "Output: (16,)\n"
     ]
    }
   ],
   "source": [
    "# Step1: Initialize the dataset with batch size of 16 and apply the data shuffle \n",
    "dataset = MyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "#Step2: Get the first batch from the dataloader. (Hint: use iterator and next functions)\n",
    "first_batch_data, first_batch_labels = next(iter(dataloader))\n",
    "\n",
    "print(\"Shape of first batch data:\\n\", first_batch_data.shape)\n",
    "print(\"Shape of first batch labels:\\n\", first_batch_labels.shape)\n",
    "\n",
    "# Step3: Initialise the Network and Perform forward propagation\n",
    "network = MyNeuralNetwork()\n",
    "output = network.forward(first_batch_data)\n",
    "print(\"Output:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf184c8",
   "metadata": {},
   "source": [
    "### Task 6: Backpropagation (Optional but recommended)\n",
    "\n",
    "The `backward()` calculates the gradients of the loss function with respect to the weights and biases using backpropagation.\n",
    "\n",
    "#### Output Layer:\n",
    "The first step is to compute the error at the output layer:\n",
    "\n",
    "$$\n",
    "dZ_3 = (\\text{output} - y_{\\text{true}}) \\cdot \\sigma'(Z_3)\n",
    "$$\n",
    "\n",
    "where $y_{\\text{true}}$ is the true label and $\\sigma'(z)$ is the derivative of the sigmoid function:\n",
    "\n",
    "$$\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$$\n",
    "\n",
    "The gradients for the weights and biases of the output layer are calculated as:\n",
    "\n",
    "$$dW_3 = dZ_3^T A_2$$\n",
    "\n",
    "$$dB_3 = \\sum dZ_3$$\n",
    "\n",
    "#### Layer 2:\n",
    "To calculate the error propagated back to the second hidden layer:\n",
    "\n",
    "$$dA_2 = dZ_3 W_3^T$$\n",
    "\n",
    "$$dZ_2 = dA_2 \\cdot \\text{ReLU}'(Z_2)$$\n",
    "\n",
    "where $\\text{ReLU}'(z)$ is defined as:\n",
    "\n",
    "$$\\text{ReLU}'(z) = \n",
    "\\begin{cases} \n",
    "1 & \\text{if } z > 0 \\\\\n",
    "0 & \\text{if } z \\leq 0 \n",
    "\\end{cases}$$\n",
    "\n",
    "The gradients for the weights and biases of the second hidden layer are:\n",
    "\n",
    "$$dW_2 = dZ_2^T A_1$$\n",
    "\n",
    "$$dB_2 = \\sum dZ_2$$\n",
    "\n",
    "#### Layer 1:\n",
    "To calculate the error propagated back to the first hidden layer:\n",
    "\n",
    "$$dA_1 = dZ_2 W_2^T$$\n",
    "\n",
    "$$dZ_1 = dA_1 \\cdot \\text{ReLU}'(Z_1)$$\n",
    "\n",
    "The gradients for the weights and biases of the first hidden layer are:\n",
    "\n",
    "$$dW_1 = dZ_1^T X$$\n",
    "\n",
    "$$dB_1 = \\sum dZ_1$$\n",
    "\n",
    "#### Updating Weights and Biases:\n",
    "Finally, the weights and biases are updated using gradient descent:\n",
    "\n",
    "$$W_3 \\leftarrow W_3 - \\eta dW_3$$\n",
    "$$B_3 \\leftarrow B_3 - \\eta dB_3$$\n",
    "\n",
    "$$W_2 \\leftarrow W_2 - \\eta dW_2$$\n",
    "$$B_2 \\leftarrow B_2 - \\eta dB_2$$\n",
    "\n",
    "$$W_1 \\leftarrow W_1 - \\eta dW_1$$\n",
    "$$B_1 \\leftarrow B_1 - \\eta dB_1$$\n",
    "\n",
    "where $\\eta$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "0b7b34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Use the existing configuration from Task 3.\n",
    "class MyNeuralNetwork:\n",
    "    def __init__(self):\n",
    "\n",
    "        # From task 3\n",
    "        self.W1 = np.random.rand(2,4)\n",
    "        self.B1 = np.random.rand(1,2)\n",
    "        \n",
    "        \n",
    "        self.W2 = np.random.rand(2,2)\n",
    "        self.B2 = np.random.rand(1,2)\n",
    "        \n",
    "        self.W3 = np.random.rand(1,2)\n",
    "        self.B3 = np.random.rand(1)\n",
    "\n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return (1 / (1 + np.exp(-z)))\n",
    "    \n",
    "    def relu_derivative(self, z):\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid_derivative(self, z):\n",
    "        s = self.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    #Use the same configuration for forward pass. Unlike the previous case, we store values of forward propagation here using class instance\n",
    "    def forward(self, X):\n",
    "        self.X = X # Store intermediate values for use in backpropagation\n",
    "\n",
    "        self.Z1 = np.matmul(X,np.transpose(self.W1)) + self.B1\n",
    "        self.A1 = self.relu(self.Z1)\n",
    "\n",
    "        self.Z2 = np.matmul(self.A1,np.transpose(self.W2)) + self.B2\n",
    "        self.A2 = self.relu(self.Z2)\n",
    "        \n",
    "        self.Z3 = np.matmul(self.A2,np.transpose(self.W3)) + self.B3\n",
    "        self.output = self.sigmoid(self.Z3)\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    def backward(self, y_true, learning_rate=0.01):\n",
    "\n",
    "        # Compute the gradients for layer 3\n",
    "        dZ3 = (self.output - y_true) * self.sigmoid_derivative(self.Z3) # Output error\n",
    "        dW3 = np.matmul(dZ3.T, self.A2) # Weight gradients for W3\n",
    "        dB3 = np.sum(dZ3, axis=0, keepdims=True) # Bias gradients for B3\n",
    "\n",
    "        # Step1: Compute the gradients for layer 2 (Hint: Check the equations for layer 2 and code from layer 3 for reference)\n",
    "        dA2 = np.matmul(dZ3, self.W3) # Propagate the error backward\n",
    "        dZ2 = dA2 * self.relu_derivative(dZ3) # Apply ReLU derivative\n",
    "        dW2 = np.matmul(dZ2.T, self.A1) # Weight gradients for W2\n",
    "        dB2 = np.sum(dZ2, axis=0, keepdims=True) # Bias gradients for B2\n",
    "\n",
    "        # Step2: Compute the gradients for layer 1 (Hint: Check the equations for layer 1 and code from layer 3 for reference)\n",
    "        dA1 = np.matmul(dZ2, self.W2) # Propagate the error backward\n",
    "        dZ1 = dA1 * self.relu_derivative(dZ2) # Apply ReLU derivative\n",
    "        dW1 = np.matmul(dZ1.T, self.A1) # Weight gradients for W1\n",
    "        dB1 = np.sum(dZ1, axis=0, keepdims=True) # Bias gradients for B1\n",
    "\n",
    "        # Step3: Update weights and biases\n",
    "        self.W3 -= learning_rate * dW3\n",
    "        self.B3 -= learning_rate * dB3.flatten()\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.B2 -= learning_rate * dB2.flatten()\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.B1 -= learning_rate * dB1.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "052af19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "===============================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2,4) (2,2) (2,4) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[116], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean((outputs \u001b[38;5;241m-\u001b[39m labels)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#5.3: Perform the backward pass (Hint: pass the true labels to the backward function in network)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m \u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Print the loss for every batch\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[115], line 70\u001b[0m, in \u001b[0;36mMyNeuralNetwork.backward\u001b[1;34m(self, y_true, learning_rate)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dW2\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB2 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dB2\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m---> 70\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mW1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdW1\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mB1 \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m dB1\u001b[38;5;241m.\u001b[39mflatten()\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (2,4) (2,2) (2,4) "
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "# Step4: Initialize the Dataset and the network\n",
    "dataset = MyDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "network = MyNeuralNetwork()\n",
    "\n",
    "# Do batch computation over the dataset\n",
    "print(f\"Epoch 1:\")\n",
    "print(\"===============================================\")\n",
    "for batch_idx, (data, labels) in enumerate(dataloader):\n",
    "    # Step5: Implement the batch training.\n",
    "    \n",
    "        # 5.1: Perform the forward pass\n",
    "        outputs = network.forward(data)\n",
    "        \n",
    "        labels = labels[:, np.newaxis]\n",
    "        \n",
    "        # 5.2: # Compute the loss (Hint: Mean Squared Error)\n",
    "        loss = np.mean((outputs - labels)**2)\n",
    "        \n",
    "        #5.3: Perform the backward pass (Hint: pass the true labels to the backward function in network)\n",
    "        network.backward(labels)\n",
    "        \n",
    "        # Print the loss for every batch\n",
    "        print(f\"Batch {batch_idx + 1} - Loss: {loss}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de44c4c",
   "metadata": {},
   "source": [
    "### Task 7: Training (Optional)\n",
    "\n",
    "Increase the number of data in the dataset (from 256x4 to 1024x4, where each class labels has 512x4) and train the above network for more number of epochs and check whether the loss is decreasing. Play around with different learning rates and see how the model behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42642486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
