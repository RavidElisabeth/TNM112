{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYyVnVvohOp_"
   },
   "source": [
    "# TNM112 -- Teaching Session 3 \n",
    "\n",
    "## Introduction to Convolutional Neural Network\n",
    "\n",
    "In this notebook, we will define and train a convolutional neural network using Keras framework. If you get stuck at any tasks, check the [keras](https://keras.io/api/) documentation for more information. The solution will be uploaded by the end of the teaching session, so that you can verify with your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZtTwkFXwaoAh"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6EvqH-LhqCV"
   },
   "source": [
    "# 1. SimpleCNN\n",
    "\n",
    "In this section, we will train a simple convolutional neural network to classify different classes in CIFAR10 dataset. Here, we will walk through the implementation steps so you can use this information to train multiple classifiers in the upcoming sections.\n",
    "\n",
    "## 1. 1. Load CIFAR10 Dataset\n",
    "\n",
    "For the dataset, we use CIFAR10. CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "In the cell below, you will load the training and test datasets separately from `keras`, convert the class labels to one-hot vectors, and print the shape of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iW9TGzEzdSm6",
    "outputId": "d47f9ead-b56e-440d-97fe-b2a1876c0b68"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "\n",
    "# load CIFAR10 from keras.datasets\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "print('=======================================================================')\n",
    "print('CIFAR10 Dataset Split:')\n",
    "print('x train shape = ', x_train.shape)\n",
    "print('y train shape = ', y_train.shape)\n",
    "print('x test shape = ', x_test.shape)\n",
    "print('y test shape = ', y_test.shape)\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# One hot encoding of class labels\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "print('=======================================================================')\n",
    "print('After One Hot Encoding:')\n",
    "print('x train shape = ', x_train.shape)\n",
    "print('y train shape = ', y_train.shape)\n",
    "print('x test shape = ', x_test.shape)\n",
    "print('y test shape = ', y_test.shape)\n",
    "print('=======================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `x_train` and `x_shape` has a shape of [N x 32 x 32 x 3], where N is the length of the corresponding dataset, 32 x 32 are the image resolution and 3 corresponds to the RGB channels of the images. \n",
    "\n",
    "**Note:** During the last teaching session, we used MNIST dataset, which were grayscale images. So the number of channels in that case was 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "768wuQzxiD6G"
   },
   "source": [
    "## 1.2. Visualize CIFAR10 data\n",
    "\n",
    "In the cell below, we use the plot functions from the previous teaching session to plot the training images, along with the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "XDqe3PfBdugU",
    "outputId": "ae35c6fa-dbd9-4a3a-f759-74118dd3e2af"
   },
   "outputs": [],
   "source": [
    "#Visualize CIFAR-10 dataset     \n",
    "def imgrid(x,y,yp,xx,yy):\n",
    "    ind = [i for i in range(x.shape[0])]\n",
    "    random.shuffle(ind)\n",
    "\n",
    "    plt.figure(figsize=(18,yy*2))\n",
    "    for i in range(xx*yy):\n",
    "        plt.subplot(yy,xx,i+1)\n",
    "        if x.shape[3]==1:\n",
    "            plt.imshow(x[ind[i],:,:,0],cmap='gray')\n",
    "        else:\n",
    "            plt.imshow(x[ind[i],:,:,:])\n",
    "\n",
    "        if len(yp)>0:\n",
    "            plt.title('p=%d, gt=%d'%(yp[ind[i]],y[ind[i]]))\n",
    "        else:\n",
    "            plt.title('label=%d'%(y[ind[i]]))\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "print('CIFAR10:')\n",
    "print('===================================================================================================================')\n",
    "imgrid(x_train,np.argmax(y_train,1),[],12,3)\n",
    "print('===================================================================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sxm21OBGiTOA"
   },
   "source": [
    "## 1.3. Model Definition\n",
    "\n",
    "In the cell below, we define a simple convolutional neural network (SimpleCNN) with two convolutional blocks using keras. \n",
    "\n",
    "**Network Architecture:**\n",
    "\n",
    "   1. **`Convolutional Layer 1:`** a Conv2D layer with 32 filters, 3x3 kernel, ReLU activation, and input shape (32, 32, 3).\n",
    "   2. **`MaxPool Layer 1:`** a MaxPooling2D layer with a 2x2 pooling window.\n",
    "   3. **`Convolutional Layer 2:`** Another Conv2D layer with 64 filters, 3x3 kernel, and ReLU activation.\n",
    "   4. **`MaxPool Layer 2:`** Another MaxPooling2D layer with a 2x2 pooling window.\n",
    "   5. **`Flatten:`** a Flatten layer to convert feature maps to a 1D vector.\n",
    "   6. **`Dense Layer 1:`** a Dense layer with 512 units and ReLU activation.\n",
    "   7. **`Output Layer:`** a Dense layer with 10 units and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GBpx914HbWFg",
    "outputId": "c64a8cf9-5a9b-4042-aa02-de30e527ed7c"
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "\n",
    "# Define a sequential model from keras and add convolutional blocks\n",
    "model = keras.Sequential(name=\"SimpleCNN\")\n",
    "\n",
    "# Add a Conv2D layer with 32 filters, 3x3 kernel, ReLU activation, and input shape (32, 32, 3).\n",
    "model.add(layers.Conv2D(32, (3,3), padding=\"same\", activation='relu', input_shape = x_train.shape[1:], name=\"Layer-1\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model.add(layers.MaxPool2D((2,2),name=\"Layer-2\"))\n",
    "\n",
    "# Add another Conv2D layer with 64 filters, 3x3 kernel, and ReLU activation.\n",
    "model.add(layers.Conv2D(64, (3,3), padding=\"same\", activation='relu', name=\"Layer-3\"))\n",
    "\n",
    "# Add another MaxPooling2D layer with a 2x2 pooling window.\n",
    "model.add(layers.MaxPool2D((2,2), name=\"Layer-4\"))\n",
    "\n",
    "# Add a Flatten layer to convert feature maps to a 1D vector.\n",
    "model.add(layers.Flatten(name=\"Layer-5\"))\n",
    "\n",
    "# Add a Dense layer with 512 units and ReLU activation.\n",
    "model.add(layers.Dense(512, activation='relu', name=\"Layer-6\"))\n",
    "\n",
    "# Add a Dense layer with 10 units and softmax activation.\n",
    "model.add(layers.Dense(10, activation='softmax', name=\"Layer-7\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pj94rQVSiwVn"
   },
   "source": [
    "## 1.4. Model Training\n",
    "\n",
    "In the cell below, the CNN will be trained based on the following hyperparameters.\n",
    "\n",
    "- **Optimizer:**  Adam optimizer\n",
    "\n",
    "- **Learning Rate:** 0.001\n",
    "\n",
    "- **Loss Function:** categorical_crossentropy\n",
    "\n",
    "- **Epochs:** 20\n",
    "\n",
    "- **Validation Split:** 20%\n",
    "\n",
    "- **Validation Frequency:** 5 (Model computes validation after every 5 epochs)\n",
    "\n",
    "- **Batch Size:** 7\n",
    "\n",
    "- **Metrics:** Accuracy\n",
    "\n",
    "**Note:** In this session, we use a validation dataset to assess whether the model is overfitting or underfitting. Instead of manually splitting the dataset for validation, we specify the validation_split parameter in Keras's `fit()` method, which automatically partitions the training dataset for validation during model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3S1noGeNgN_C",
    "outputId": "4b53cabd-9860-4fae-a2dc-4a1de70993fb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer with learning rate\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "# Compile the model for training\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Model training and store the trainings result in a log\n",
    "log = model.fit(x_train, y_train, batch_size=256, epochs=20,\n",
    "          validation_split=0.2, validation_freq=5,verbose=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Plot the performance curves\n",
    "\n",
    "We define a function `plot_curve` to visualize the training loss and accuracy from the above training process using `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to plot loss and accuracy curves\n",
    "def plot_curve(log):\n",
    "    N_train = len(log.history['loss'])\n",
    "    N_valid = len(log.history['val_loss'])\n",
    "    \n",
    "    plt.figure(figsize=(18,4))\n",
    "    \n",
    "    # Plot loss on training and validation set\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(log.history['loss'])\n",
    "    plt.plot(np.linspace(0,N_train-1,N_valid), log.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid('on')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    \n",
    "    # Plot accuracy on training and validation set\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(log.history['accuracy'])\n",
    "    plt.plot(np.linspace(0,N_train-1,N_valid), log.history['val_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid('on')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_curve(log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa_r8EXrjHV_"
   },
   "source": [
    "## 1.6. Evaluation\n",
    "\n",
    "In the cell below, the SimpleCNN is evaluated on the test set by computing the Mean Squared Error loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFT1iNlvjKjN",
    "outputId": "e7b531cb-28ab-484f-b015-c4ec33f80d93"
   },
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('=======================================================================')\n",
    "\n",
    "# The first element in the score contains the MSE and second element contains the accuracy.   \n",
    "print(\"Test loss:     \", score[0])\n",
    "print(\"Test accuracy: \", 100*score[1])\n",
    "print('=======================================================================')\n",
    "\n",
    "# (Optional) You are free to compute other metrics (Precision, Recall and F1-score) below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Addressing Overfitting and Improve Model Performance\n",
    "\n",
    "In this section, you will explore various techniques to address the problem of overfitting in Machine Learning and to improve the performance of machine learning model.\n",
    "\n",
    "\n",
    "## Task 2.1. Adding Dropout\n",
    "\n",
    "The first technique involves using dropout. In the following cell, you will modify the previously implemented SimpleCNN by incorporating dropout layers.\n",
    "\n",
    "**Network Architecture**:\n",
    "\n",
    "   1. **`Convolutional Layer 1:`** a Conv2D layer with 32 filters, 3x3 kernel, ReLU activation, and input shape (32, 32, 3).\n",
    "   2. **`MaxPool Layer 1:`** a MaxPooling2D layer with a 2x2 pooling window.\n",
    "   3. **`Dropout Layer 1:`** a Dropout layer with the rate of 0.25\n",
    "   4. **`Convolutional Layer 2:`** Another Conv2D layer with 64 filters, 3x3 kernel, and ReLU activation.\n",
    "   5. **`MaxPool Layer 2:`** Another MaxPooling2D layer with a 2x2 pooling window.\n",
    "   6. **`Dropout Layer 2:`** Another Dropout layer with the rate of 0.25\n",
    "   7. **`Flatten:`** a Flatten layer to convert feature maps to a 1D vector.\n",
    "   8. **`Dense Layer 1:`** a Dense layer with 512 units and ReLU activation.\n",
    "   9. **`Dropout Layer 3:`** Another Dropout layer with the rate of 0.25\n",
    "   10. **`Output Layer:`** a Dense layer with 10 units and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model from keras and add convolutional blocks\n",
    "model1 = keras.Sequential(name=\"SimpleCNN_with_Dropout\")\n",
    "\n",
    "# Add a Conv2D layer with 32 filters, 3x3 kernel, ReLU activation, and input shape (32, 32, 3).\n",
    "model1.add(layers.Conv2D(32, (3,3), padding=\"same\", activation='relu', input_shape = x_train.shape[1:], name=\"Layer-1\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model1.add(layers.MaxPool2D((2,2),name=\"Layer-2\"))\n",
    "\n",
    "# Add a Dropout layer with the rate of 0.25\n",
    "model1.add(layers.Dropout(0.25, name='Layer-3'))\n",
    "\n",
    "# Add another Conv2D layer with 64 filters, 3x3 kernel, and ReLU activation.\n",
    "model1.add(layers.Conv2D(64, (3,3), padding=\"same\", activation='relu', name=\"Layer-4\"))\n",
    "\n",
    "# Add another MaxPooling2D layer with a 2x2 pooling window.\n",
    "model1.add(layers.MaxPool2D((2,2), name=\"Layer-5\"))\n",
    "\n",
    "# Add another Dropout layer with the rate of 0.25\n",
    "model1.add(layers.Dropout(0.25, name='Layer-6'))\n",
    "\n",
    "# Add a Flatten layer to convert feature maps to a 1D vector.\n",
    "model1.add(layers.Flatten(name=\"Layer-7\"))\n",
    "\n",
    "# Add a Dense layer with 512 units and ReLU activation.\n",
    "model1.add(layers.Dense(512, activation='relu', name=\"Layer-8\"))\n",
    "\n",
    "# Add another Dropout layer with the rate of 0.25\n",
    "model1.add(layers.Dropout(0.5, name='Layer-9'))\n",
    "\n",
    "# Add a Dense layer with 10 units and softmax activation.\n",
    "model1.add(layers.Dense(10, activation='softmax', name=\"Output_Layer\"))\n",
    "\n",
    "model1.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the model for training\n",
    "model1.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "log1 = model1.fit(x_train, y_train, batch_size=256, epochs=20,\n",
    "          validation_split=0.2, validation_freq=5, verbose=True);\n",
    "\n",
    "#Plot Loss and Accuracy curves\n",
    "plot_curve(log1)\n",
    "\n",
    "#Displaying the test performance\n",
    "score1 = model1.evaluate(x_test, y_test, verbose=0)\n",
    "print('=======================================================================')\n",
    "print(\"Test loss:     \", score1[0])\n",
    "print(\"Test accuracy: \", 100*score1[1])\n",
    "print('=======================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2. More Layers and Kernels\n",
    "\n",
    "The next approach is to expand the current model by increasing the number of layers and adding more kernels (filters). In the cell below, you will design a deeper Convolutional Neural Network with additional layers and a greater number of kernels compared to SimpleCNN.\n",
    "\n",
    "**Network Architecture:**\n",
    "\n",
    "1. **`Convolutional Layer 1:`** A Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, and input shape (32, 32, 3).\n",
    "2. **`Convolutional Layer 2:`** Another Conv2D layer with 64 filters, 3x3 kernel, and ReLU activation.\n",
    "3. **`MaxPool Layer 1:`** A MaxPooling2D layer with a 2x2 pooling window.\n",
    "4. **`Dropout Layer 1:`** A Dropout layer with a rate of 0.25.\n",
    "5. **`Convolutional Layer 3:`** A Conv2D layer with 128 filters, 3x3 kernel, and ReLU activation.\n",
    "6. **`Convolutional Layer 4:`** Another Conv2D layer with 128 filters, 3x3 kernel, and ReLU activation.\n",
    "7. **`MaxPool Layer 2:`** A MaxPooling2D layer with a 2x2 pooling window.\n",
    "8. **`Dropout Layer 2:`** A Dropout layer with a rate of 0.25.\n",
    "9. **`Flatten:`** A Flatten layer to convert feature maps to a 1D vector.\n",
    "10. **`Dense Layer 1:`** A Dense layer with 512 units and ReLU activation.\n",
    "11. **`Dropout Layer 3:`** A Dropout layer with a rate of 0.5.\n",
    "12. **`Output Layer:`** A Dense layer with 10 units and softmax activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model from keras and add convolutional blocks\n",
    "model2 = keras.Sequential(name=\"Deeper_CNN\")\n",
    "\n",
    "# Add a Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, and input shape (32, 32, 3).\n",
    "model2.add(layers.Conv2D(64, (3,3), padding=\"same\", activation='relu', input_shape = x_train.shape[1:], name=\"Layer-1\"))\n",
    "\n",
    "# Add another Conv2D layer with 64 filters, 3x3 kernel, and ReLU activation.\n",
    "model2.add(layers.Conv2D(64, (3,3), padding=\"same\", activation='relu', name=\"Layer-2\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model2.add(layers.MaxPool2D((2,2),name=\"Layer-3\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.25.\n",
    "model2.add(layers.Dropout(0.25, name='Layer-4'))\n",
    "\n",
    "# Add a Conv2D layer with 128 filters, 3x3 kernel, and ReLU activation.\n",
    "model2.add(layers.Conv2D(128, (3,3), padding=\"same\", activation='relu', name=\"Layer-5\"))\n",
    "\n",
    "# Add another Conv2D layer with 128 filters, 3x3 kernel, and ReLU activation.\n",
    "model2.add(layers.Conv2D(128, (3,3), padding=\"same\", activation='relu', name=\"Layer-6\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model2.add(layers.MaxPool2D((2,2), name=\"Layer-7\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.25.\n",
    "model2.add(layers.Dropout(0.25, name='Layer-8'))\n",
    "\n",
    "# Add a Flatten layer to convert feature maps to a 1D vector.\n",
    "model2.add(layers.Flatten(name=\"Layer-9\"))\n",
    "\n",
    "# Add a Dense layer with 512 units and ReLU activation.\n",
    "model2.add(layers.Dense(512, activation='relu', name=\"Layer-10\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.5.\n",
    "model2.add(layers.Dropout(0.5, name='Layer-11'))\n",
    "\n",
    "# Add a Dense layer with 10 units and softmax activation.\n",
    "model2.add(layers.Dense(10, activation='softmax', name=\"Output_Layer\"))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configure the model for training\n",
    "model2.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "log2 = model2.fit(x_train, y_train, batch_size=256, epochs=20,\n",
    "          validation_split=0.2, validation_freq=5, verbose=True);\n",
    "\n",
    "#Plot Loss and Accuracy curves\n",
    "plot_curve(log2)\n",
    "\n",
    "#Displaying the test performance\n",
    "score2 = model2.evaluate(x_test, y_test, verbose=0)\n",
    "print('=======================================================================')\n",
    "print(\"Test loss:     \", score2[0])\n",
    "print(\"Test accuracy: \", 100*score2[1])\n",
    "print('=======================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3. Batch Normalization\n",
    "\n",
    "The next technique involves using Batch Normalization to improve the training process and stabilize the learning. In the cell below, you will add BatchNormalization layers to the model, which will normalize the activations and gradients, helping to accelerate training and reduce sensitivity to initialization.\n",
    "\n",
    "**Network Architecture:**\n",
    "\n",
    "1. **`Convolutional Layer 1:`** A Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, He uniform kernel initializer, and input shape (32, 32, 3).  \n",
    "2. **`Batch Normalization 1:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "3. **`Convolutional Layer 2:`** Another Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.  \n",
    "4. **`Batch Normalization 2:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "5. **`MaxPool Layer 1:`** A MaxPooling2D layer with a 2x2 pooling window.  \n",
    "6. **`Dropout Layer 1:`** A Dropout layer with a rate of 0.3.  \n",
    "7. **`Convolutional Layer 3:`** A Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.  \n",
    "8. **`Batch Normalization 3:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "9. **`Convolutional Layer 4:`** Another Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.  \n",
    "10. **`Batch Normalization 4:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "11. **`MaxPool Layer 2:`** A MaxPooling2D layer with a 2x2 pooling window.  \n",
    "12. **`Dropout Layer 2:`** A Dropout layer with a rate of 0.4.  \n",
    "13. **`Flatten:`** A Flatten layer to convert feature maps to a 1D vector.  \n",
    "14. **`Dense Layer 1:`** A Dense layer with 256 units, ReLU activation, and He uniform kernel initializer.  \n",
    "15. **`Batch Normalization 5:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "16. **`Dropout Layer 3:`** A Dropout layer with a rate of 0.5.  \n",
    "17. **`Output Layer:`** A Dense layer with 10 units and softmax activation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a sequential model from keras and add convolutional blocks\n",
    "model3 = keras.Sequential(name=\"CNN_with_BatchNorm\")\n",
    "\n",
    "# Add a Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, He uniform kernel initializer, and input shape (32, 32, 3).\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',input_shape = x_train.shape[1:],name=\"Layer-1\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model3.add(layers.BatchNormalization(name=\"Layer-2\"))\n",
    "\n",
    "# Add another Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.\n",
    "model3.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',name=\"Layer-3\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model3.add(layers.BatchNormalization(name=\"Layer-4\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model3.add(layers.MaxPool2D((2, 2),name=\"Layer-5\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.3.\n",
    "model3.add(layers.Dropout(0.3,name=\"Layer-6\"))\n",
    "\n",
    "# Add a Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.\n",
    "model3.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',name=\"Layer-7\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model3.add(layers.BatchNormalization(name=\"Layer-8\"))\n",
    "\n",
    "# Add another Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.\n",
    "model3.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',name=\"Layer-9\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model3.add(layers.BatchNormalization(name=\"Layer-10\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model3.add(layers.MaxPool2D((2, 2),name=\"Layer-11\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.4.\n",
    "model3.add(layers.Dropout(0.4,name=\"Layer-12\"))\n",
    "\n",
    "# Add a Flatten layer to convert feature maps to a 1D vector.\n",
    "model3.add(layers.Flatten(name=\"Layer-13\"))\n",
    "\n",
    "# Add a Dense layer with 256 units, ReLU activation, and He uniform kernel initializer.\n",
    "model3.add(layers.Dense(256, activation='relu', kernel_initializer='he_uniform',name=\"Layer-14\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model3.add(layers.BatchNormalization(name=\"Layer-15\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.5.\n",
    "model3.add(layers.Dropout(0.5,name=\"Layer-16\"))\n",
    "\n",
    "# Add a Dense layer with 10 units and softmax activation.\n",
    "model3.add(layers.Dense(10, activation='softmax',name='Output_Layer'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Configure the model for training\n",
    "model3.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "log3 = model3.fit(x_train, y_train, batch_size=256, epochs=30,\n",
    "          validation_split=0.2, validation_freq=5, verbose=True);\n",
    "\n",
    "#Plot Loss and Accuracy curves\n",
    "plot_curve(log3)\n",
    "\n",
    "#Displaying the test performance\n",
    "score3 = model3.evaluate(x_test, y_test, verbose=0)\n",
    "print('=======================================================================')\n",
    "print(\"Test loss:     \", score3[0])\n",
    "print(\"Test accuracy: \", 100*score3[1])\n",
    "print('=======================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As we are adding more layers and parameters to the network, we will increase the number of epochs to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.4. Data Augmentation\n",
    "\n",
    "The next approach involves applying data augmentation techniques to enhance the diversity of the training data. In the cell below, you will implement random rotation, translation, flipping, and contrast adjustments to augment the dataset and improve the model’s generalization.\n",
    "\n",
    "**Network Architecture:**\n",
    "\n",
    "1. **`Augmentation 1:`** A RandomRotation layer with a rotation factor of 0.05.  \n",
    "2. **`Augmentation 2:`** A RandomTranslation layer with horizontal and vertical translation of 0.1.  \n",
    "3. **`Augmentation 3:`** A RandomFlip layer with horizontal flipping.  \n",
    "4. **`Augmentation 4:`** A RandomContrast layer with a contrast adjustment factor of 0.2.  \n",
    "5. **`Convolutional Layer 1:`** A Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, He uniform kernel initializer, and input shape (32, 32, 3).  \n",
    "6. **`Batch Normalization 1:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "7. **`Convolutional Layer 2:`** Another Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.  \n",
    "8. **`Batch Normalization 2:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "9. **`MaxPool Layer 1:`** A MaxPooling2D layer with a 2x2 pooling window.  \n",
    "10. **`Dropout Layer 1:`** A Dropout layer with a rate of 0.3.  \n",
    "11. **`Convolutional Layer 3:`** A Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.  \n",
    "12. **`Batch Normalization 3:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "13. **`Convolutional Layer 4:`** Another Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.  \n",
    "14. **`Batch Normalization 4:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "15. **`MaxPool Layer 2:`** A MaxPooling2D layer with a 2x2 pooling window.  \n",
    "16. **`Dropout Layer 2:`** A Dropout layer with a rate of 0.4.  \n",
    "17. **`Flatten:`** A Flatten layer to convert feature maps to a 1D vector.  \n",
    "18. **`Dense Layer 1:`** A Dense layer with 256 units, ReLU activation, and He uniform kernel initializer.  \n",
    "19. **`Batch Normalization 5:`** A BatchNormalization layer to normalize the activations of the previous layer.  \n",
    "20. **`Dropout Layer 3:`** A Dropout layer with a rate of 0.5.  \n",
    "21. **`Output Layer:`** A Dense layer with 10 units and softmax activation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a sequential model from keras and add convolutional blocks\n",
    "model4 = keras.Sequential(name=\"CNN_with_Data_Aug\")\n",
    "\n",
    "# Add an Input Layer for Data Augmentation\n",
    "model4.add(layers.InputLayer(x_train.shape[1:]))\n",
    "\n",
    "# Add a RandomRotation layer with a rotation factor of 0.05.\n",
    "model4.add(layers.RandomRotation(factor=0.05,name='Aug-1'))\n",
    "\n",
    "# Add a RandomTranslation layer with horizontal and vertical translation of 0.1.\n",
    "model4.add(layers.RandomTranslation(0.1,0.1,name='Aug-2'))\n",
    "\n",
    "# Add a RandomFlip layer with horizontal flipping.\n",
    "model4.add(layers.RandomFlip(mode='horizontal',name='Aug-3'))\n",
    "\n",
    "# Add a RandomContrast layer with a contrast adjustment factor of 0.2.\n",
    "model4.add(layers.RandomContrast(factor=0.2,name='Aug-4'))\n",
    "\n",
    "# Add a Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, He uniform kernel initializer, and input shape (32, 32, 3).\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',input_shape = x_train.shape[1:],name=\"Layer-1\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model4.add(layers.BatchNormalization(name=\"Layer-2\"))\n",
    "\n",
    "# Add another Conv2D layer with 64 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.\n",
    "model4.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',name=\"Layer-3\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model4.add(layers.BatchNormalization(name=\"Layer-4\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model4.add(layers.MaxPool2D((2, 2),name=\"Layer-5\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.3.\n",
    "model4.add(layers.Dropout(0.3,name=\"Layer-6\"))\n",
    "\n",
    "# Add a Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.\n",
    "model4.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',name=\"Layer-7\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model4.add(layers.BatchNormalization(name=\"Layer-8\"))\n",
    "\n",
    "# Add another Conv2D layer with 128 filters, 3x3 kernel, ReLU activation, and He uniform kernel initializer.\n",
    "model4.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same',name=\"Layer-9\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model4.add(layers.BatchNormalization(name=\"Layer-10\"))\n",
    "\n",
    "# Add a MaxPooling2D layer with a 2x2 pooling window.\n",
    "model4.add(layers.MaxPool2D((2, 2),name=\"Layer-11\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.4.\n",
    "model4.add(layers.Dropout(0.4,name=\"Layer-12\"))\n",
    "\n",
    "# Add a Flatten layer to convert feature maps to a 1D vector.\n",
    "model4.add(layers.Flatten(name=\"Layer-13\"))\n",
    "\n",
    "# Add a Dense layer with 256 units, ReLU activation, and He uniform kernel initializer.\n",
    "model4.add(layers.Dense(256, activation='relu', kernel_initializer='he_uniform',name=\"Layer-14\"))\n",
    "\n",
    "# Add a BatchNormalization layer to normalize the activations of the previous layer.\n",
    "model4.add(layers.BatchNormalization(name=\"Layer-15\"))\n",
    "\n",
    "# Add a Dropout layer with a rate of 0.5.\n",
    "model4.add(layers.Dropout(0.5,name=\"Layer-16\"))\n",
    "\n",
    "# Add a Dense layer with 10 units and softmax activation.\n",
    "model4.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Configure the model for training\n",
    "model4.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[\"accuracy\"])\n",
    "log4 = model4.fit(x_train, y_train, batch_size=256, epochs=50,\n",
    "          validation_split=0.2, validation_freq=5, verbose=True);\n",
    "\n",
    "#Plot Loss and Accuracy curves\n",
    "plot_curve(log4)\n",
    "\n",
    "#Displaying the test performance\n",
    "score4 = model4.evaluate(x_test, y_test, verbose=0)\n",
    "print('=======================================================================')\n",
    "print(\"Test loss:     \", score4[0])\n",
    "print(\"Test accuracy: \", 100*score4[1])\n",
    "print('=======================================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** As we are adding more layers and parameters to the network, we will increase the number of epochs to achieve better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Display the results\n",
    "\n",
    "In the following cell, we will display the results from various models trained during the session. We will assess the models using the test set to obtain accuracy and loss, and present them in a bar graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\"Accuracy\":[100*score[1],100*score1[1],100*score2[1],100*score3[1],100*score4[1]],\n",
    "          \"Loss\":[score[0],score1[0],score2[0],score3[0],score4[0]]}\n",
    "\n",
    "df = pd.DataFrame(results, index = [\"SimpleCNN\", \"SimpleCNN with Dropout\", \"Deeper CNN\", \"CNN with BatchNorm\", \"CNN with Data Aug\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"SimpleCNN\", \"Dropout\", \"DCNN\", \"BatchNorm\", \"DataAug\"]\n",
    "acc = results[\"Accuracy\"]\n",
    "loss = results[\"Loss\"]\n",
    "\n",
    "fig = plt.figure(figsize =(15, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "# creating the bar plot\n",
    "plt.bar(models, acc, color ='red', \n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "# creating the bar plot\n",
    "plt.bar(models, loss, color ='blue', \n",
    "        width = 0.4)\n",
    "plt.xlabel(\"Models\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.suptitle(\"Model Performance\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl4mt",
   "language": "python",
   "name": "dl4mt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
